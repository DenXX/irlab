% chap2.tex
%

\mychapter{Related Work}
\label{chapter:related}

\noindent

The field of automatic questions answering has a long history of research and dates back to the days when the first computers appear.
By the early 60s people have already explored multiple different approaches to question answering and a number of text-based and knowledge base QA systems existed at that time \cite{Simmons:1965:AEQ:363707.363732,Simmons:1970:NLQ:361953.361963}.
In 70s and 80s the development of restricted domain knowledge bases and computational linguistics theories facilitated the development of interactive expert and text comprehension systems \cite{androutsopoulos1995natural,shortliffe1975model,woods1977lunar,wilensky1988berkeley}.
The modern era of question answering research was motivated by a series of Text Retrieval Conference (TREC\footnote{http://trec.nist.gov}) question answering shared tasks, which was organized annually since 1999 \cite{voorhees2001trec}.
A comprehensive survey of the approaches from TREC QA 2007 can be found in \cite{dang2007overview}.
An interested reader can refer to a number of surveys to track the progress made in automatic question answering over the years  \cite{hirschman2001natural,andrenucci2005automated,wang2006survey,Kolomiyets:2011:SQA:2046840.2047162,prager2006open,allam2012question,gupta2012survey}.
% There are a number of works, focusing on the future research directions in QA, \eg \cite{burger2001issues}.

The main focus of research in automatic question answering was on factoid questions.
However, recently we can observe an increased interest in non-factoid question answering, and as an indicator in 2015 TREC started a LiveQA shared task track\footnote{http://trec-liveqa.org/}, in which the participant systems had to answer various questions coming from real users of Yahoo! Answers\footnote{http://answers.yahoo.com/} in real time.

In the rest of the chapter I will describe related work in factoid (Section \ref{sec:rel_work:factoid}) and non-factoid (Section \ref{sec:rel_work:nonfactoid}) question answering with the focus on data sources used, and Section \ref{sec:rel_work:user} provides an overview on some existing research on user interaction with QA systems.

\section{Factoid question answering}
\label{sec:rel_work:factoid}

Since the early days of automatic question answering researches explored different sources of data, which lead to the development of two major approaches to factoid question answering: text-based (TextQA) and knowledge base question answering (KBQA) \cite{Simmons:1965:AEQ:363707.363732}.
We will first describe related work in TextQA (Section \ref{sec:rel_work:factoid:text}), then introduce KBQA (Section \ref{sec:rel_work:factoid:kb}) and in Section \ref{sec:rel_work:factoid:hybrid} present existing techniques for combining different information sources together.

\subsection{Text-based question answering}
\label{sec:rel_work:factoid:text}

A traditional approach to factoid question answering over text document collections, popularized by TREC QA task, starts by querying a collection with possibly transformed question and retrieving a set of potentially relevant documents, which are then used to identify the answer.
Information retrieval for question answering has certain differences from traditional IR methods \cite{keikha2014retrieving}, which are usually based on keyword matches.
A natural language question contains certain information, that is not expected to be present in the answer (\eg the keyword who, what, when, \etc), and the answer statement might use language that is different from the question (lexical gap problem).
On the other side, there is a certain additional information about expected answer statement, that a QA system might infer from the question (\eg we expect to see in a number in response to the ``how many'' question).
One way to deal with this problem is to transform the question in certain ways before querying a collection \cite{AgichteinLG01,brill_askmsr}.
Raw text data might be extended with certain semantic annotations by applying part of speech tagger, semantic role labeling, named entity recognizer, \etc.
By indexing these annotations a question answering system gets an opportunity to query collection with additional attributes, inferred from the question \cite{bilotti2007structured,yao2013automatic}.

The next stage in TextQA is to select sentences, that might contain the answer.
One of the mostly used benchmark datasets for the task, proposed in \cite{wang2007jeopardy}, is based on TREC QA questions and sentences retrieved by participating systems\footnote{A table with all known benchmark results and links to the corresponding papers can be found on http://aclweb.org/aclwiki/index.php?title=Question\_Answering\_(State\_of\_the\_art)}.
The early approaches for the task used simple keyword match strategies \cite{ittycheriah2001ibm,soubbotin2001patterns}.
However, in many cases keywords doesn't capture the similarity in meaning of the sentences very well and researches started looking on syntactic information.
Syntactic and dependency tree edit distances and kernels allow to measure the similarity between the structures of the sentences \cite{punyakanok2004mapping,shen2005exploring,heilman2010tree,yao2013answer,wang2010probabilistic}.
Recent improvements on the answer sentence selection task come are associated with the deep learning techniques, \eg recursive neural networks using sentence dependency tree \cite{iyyer2014neural}, convolutional neural networks \cite{yu2014deep,santos2016attentive}, recurrent neural networks \cite{tan2015lstm,WangN15}.
Another dataset, called WikiQA \cite{yang2015wikiqa}, raises a problem of answer triggering, \ie detecting cases when the retrieved set of sentences don't contain the answer.

To provide a user with the concise answer to his factoid question QA systems extract the actual answer phrase from retrieved sentences.
This problem is often formulated as a sequence labeling problem, which can be solved using structured prediction models, such as CRF \cite{yao2013answer}, or as a node labeling problem in an answer sentence parse tree \cite{malon2013answer}.

Unfortunately, passages include very limited amount of information about the candidate answer entities, \ie very often it doesn't include the information about their types (person, location, organization, or more fine-grained CEO, president, basketball player, \etc), which is very important to answer question correctly, \eg for the question ``\textit{what country will host the 2016 summer olympics?}'' we need to know that \texttt{Rio de Janeiro} is a city and \texttt{Brazil} is the country and the correct answer to the question.
Therefore, a lot of effort has been put into developing answer type typologies \cite{hovy2000question,Hovy:2002:QTS:1289189.1289206} and predicting and matching expected and candidate answer types from the available data \cite{LiRoth02,li2006learning, prager2006question}.
Many approaches exploited external data for this task, I will describe some of this efforts in Section \ref{sec:rel_work:factoid:hybrid}.

Very large text collections, such as the Web, contain many documents expressing the same information, which makes it possible to use a simpler techniques and rely on redundancy of the information.
\texttt{AskMSR} QA system was one of the first to exploit this idea, and achieved very impressive results on TREC QA 2001 shared task \cite{brill2002analysis}.
The system starts by transforming a question into search queries, extracts snippets of search results from a web search engine, and consider word n-grams as answer candidates, ranking them by frequency.
A recent revision of the AskMSR QA system \cite{tsai2015web} introduced several improvements to the original system, \ie named entity tagger for candidate extraction, and additional semantic similarity features for answer ranking.
It was also observed, that modern search engines are much better in returning the relevant documents for question queries and query generation step is no longer needed.
Another notable systems, that used the web as the source for question answering are \texttt{MULDER}\cite{kwok2001scaling}, \texttt{Aranea} \cite{lin2003question}, and a detailed analysis of what affects the performance of the redundancy-based question answering systems can be found in \cite{lin2007exploration}.

\subsection{Knowledge base question answering}
\label{sec:rel_work:factoid:kb}

Earlier in the days knowledge bases were relatively small and contained information specific to a particular domain, \eg baseball statistics \cite{green1961baseball}, lunar geology \cite{woods1977lunar}, geography \cite{zelle1996learning}.
However, one of the main challenges in KBQA is mapping between natural language phrases to the database concepts, which raises a problem of domain adaption of question answering systems.

Recent development of large scale knowledge bases (\eg dbPedia \cite{auer2007dbpedia}, Freebase \cite{Bollacker:2008:FCC:1376616.1376746}, YAGO \cite{suchanek2007yago}, WikiData\footnote{http://www.wikidata.org} shifted the attention towards open domain question answering.
Knowledge base question answering approaches can be evaluated on an annual Question Answering over Linked Data (QALD\footnote{www.sc.cit-ec.uni-bielefeld.de/qald/}) shared task, and some popular benchmark dataset, such as Free917 \cite{cai2013large} and WebQuestions \cite{BerantCFL13:sempre}.
A survey of some of the proposed approaches can be found in \cite{unger2014introduction}.

A series of QALD evaluation campaigns has started in 2011, and since then a number of different subtasks have been offered, \ie since QALD-3 includes a multilingual task, and QALD-4 formulated a problem of hybrid question answering.
These tasks usually use dbPedia knowledge base and provide a training set of questions, annotated with the ground truth SPARQL queries.
The hybrid track is of particular interest to the topic of this dissertation, as the main goal in this task is to use both structured RDF triples and free form text available in dbPedia abstracts to answer user questions.

% DIFFERENCES OF HYBRID TRACK AND THIS WORK
% Questions in the hybrid track are manually created in such a way, that they can \textit{only} be answered using a combination of RDF and free text data.
% Secondly, the hybrid task focuses on text data already present in a KB, whereas we are exploring external text resources.
% In general, because of the expensive labeling process, QALD datasets are rather small, for example, QALD-5 training set for multilingual question answering includes 300 examples and 40 examples for the hybrid task.
% The evaluation was performed on 50 questions for multilingual task and just 10 for hybrid.

The problem of lexical gap and lexicon construction for mapping natural language phrases to knowledge base concepts is one of the major difficulties in KBQA.
The earlier systems were mainly trained from question annotated with the correct parse logical form, which is expensive to obtain.
Such an approach is hard to scale to large open domain knowledge bases, which contain millions of entities and thousands of different predicates.
An idea to extend a trained parser with additional lexicon, built from the Web and other resources, has been proposed by \cite{CaiY13}.
However, most of the parses of the question produce different results, which means that it is possible to use question-answer pairs directly \cite{BerantCFL13:sempre}.
PARALEX system (\cite{fader2013paraphrase}) construct a lexicon from a collection of question paraphrases from WikiAnswers\footnote{https://answers.wikia.com/}.
A somewhat backwards approach was proposed in ParaSempre model of \cite{BerantL14:parasempre}, which ranks candidate structured queries by first constructing a canonical utterance for each query and then using a paraphrasing model to score it against the original question.
Another approach to learn term-predicate mapping is to use patterns obtained using distant supervision \cite{MintzBSJ09} labeling of a large text corpus, such as ClueWeb \cite{yao2014freebase}.
Such labelled collections can also be used to train a KBQA system, as demonstrated by \cite{ReddyLS14}.
Such an approach is very attractive as it doesn't require any manual labeling and can be easily transfered to a new domain.
However, learning from statements instead of question answer pairs has certain disadvantages, \eg question-answer lexical gap and noise in distant supervision labeling.
Modern knowledge bases also contain certain surface forms for their predicates and entities, which makes it possible to convert KB RDF triples into questions and use them for training \cite{BordesCW14:emnlp}.
Finally, many systems work with distributed vector representations for words and RDF triples and use various deep learning techniques for answer selection.
A common strategy is to use a joint embedding of text and knowledge base concepts.
For example, character n-gram text representation as input to a convolutional neural network can capture the gist of the question and help map phrases to entities and predicates \cite{yih2014semantic}.
Joint embeddings can be trained using multi-task learning, \eg a system can learn to embed a question and candidate answer subgraph using question-answer pairs and question paraphrases at the same time (\cite{BordesCW14:emnlp}).
Memory Networks, developed by the Facebook AI Lab, can also be used to return triples stored in network memory in a response to the user question \cite{bordes2015large}.
This approach uses embeddings of predicates and can answer relatively simple questions, that do not contain any constraints and aggregations.
To extend deep learning framework to more complex questions, \cite{dong2015question} use multi-column convolutional neural network to capture the embedding of the entity path, context and type.

As for the architecture of KBQA systems, two major approaches have been identified: semantic parsing and information extraction.
Semantic parsing starts from question utterances and work to produce the corresponding semantic representation, \eg logical form.
The model of \cite{BerantCFL13:sempre} uses a CCG parser, which can produce many candidates on each level of parsing tree construction.
A common strategy is to use beam search to keep top-k options on each parsing level or agenda-based parsing \cite{berant2015imitation}, which maintains current best parses across all levels.
An alternative information extraction strategy was proposed by \cite{YaoD14}, which can be very effective for relatively simple questions.
A comparison of this approaches can be found in \cite{yao2014freebase}.
The idea of the information extraction approach is that for most of the questions the answer lies in the neighborhood of the question topic entity.
Therefore, it is possible to use a relatively small set of query patterns to generate candidate answers, which are then ranked using the information about how well involved predicates and entities match the original question.

Question entity identification and disambiguation is the key component in such systems, they cannot answer the question correctly if the question entity isn't identified.
Different systems used NER to tag question entities, which are then linked to a knowledge base using a lexicon of entity names \cite{BerantCFL13:sempre,BerantL14:parasempre,xu2014answering}.
However, NER can easily miss the right span and the whole system would fail to produce the answer.
Recently, most of the state-of-the-art system on WebQuestions dataset used a strategy to consider a reasonable subset of token n-grams, each of which can map to zero or more KB entities, which are disambiguated on the answer ranking stage \cite{yao-scratch-qa-naacl2015,bastmore:cikm:2015:aquu,yih:ACL:2015:STAGG}.
Ranking of candidates can be done using a simple linear classification model \cite{yao-scratch-qa-naacl2015} or a more complex gradient boosted trees ranking model \cite{bastmore:cikm:2015:aquu,yih:ACL:2015:STAGG}.

Some questions contain certain conditions, that require special filters or aggregations to be applied to a set of entities. 
For example, the question ``\textit{who won 2011 heisman trophy?}'' contains a date, that needs to be used to filter the set of heisman trophy winners, the question ``\textit{what high school did president bill clinton attend?}'' requires a filter on the entity type to filter high schools from the list of educational institutions, and ``\textit{what is the closest airport to naples florida?}'' requires a set of airports to be sorted by distance and the closest one to be selected.
Information extraction approaches either needs to extend the set of candidate query templates used, which is usually done manually, or to attach such aggregations later in the process, after the initial set of entities have been extracted \cite{yih:ACL:2015:STAGG}.
An alternative strategy to answer complex questions is to extend RDF triples as a unit of knowledge with additional arguments and perform question answering over n-tuples \cite{yin2015answering}.
In \cite{wang2015large} authors propose to start from single KB facts and build more complex logical formulas by combining existing ones, while scoring candidates using paraphrasing model.
Such a a template-free model combines the benefits of semantic parsing and information extraction approaches.

% PROPOSAL
% However, most of the models are still biased towards the types of questions present in the training set and would benefit from more training data.
% In this work I propose to extend the training set with question-answer pairs available on CQA websites, which were shown to be useful for relation extraction \cite{SavenkovLDA15}.
% In addition, I propose to use unlabeled text resources for candidate query ranking, which can help to generalize to unseen types of questions and questions about predicates never mentioned in the training set.

\subsection{Hybrid question answering}
\label{sec:rel_work:factoid:hybrid}

A natural idea of combining available information sources to improve question answering has been explored for a long time.
WordNet lexical database \cite{miller1995wordnet} was among the first resources, that were adapted by QA community \cite{hovy2001use,pasca2001informative}, and it was used for such tasks as query expansion and definition extraction.
Wikipedia\footnote{http://www.wikipedia.org}, which can be characterized as an unstructured and semi-structured (infoboxes) knowledge base, quickly became a valuable resource for answer extraction and verification \cite{ahn2005using,buscaldi2006mining}.
Developers of the Aranea QA system noticed that structured knowledge bases are very effective in answering a significant portion of relatively simple questions \cite{lin2003question}.
They designed a set of regular expressions for popular questions that can be efficiently answered from a knowledge base and fall back to regular text-based methods for the rest of the questions.

One of the major drawbacks of knowledge bases is their incompleteness, which means that many entities, predicates and facts are missing from knowledge bases, which limits the number of questions one can answer using them.
One approach to increase the coverage of knowledge bases is to extract information from other resources, such as raw text\cite{MintzBSJ09,jijkoun2004information,Gupta:2014:BOS:2732286.2732288}, web tables \cite{Cafarella:2008:WEP:1453856.1453916}, \etc.
However, the larger the knowledge base gets, the more difficult it's to find a mapping from natural language phrases to KB concepts.
Alternatively, open information extraction techniques (\cite{Etzioni:2008:OIE:1409360.1409378}) can be used to extract a surface form-based knowledge base, which can be very effective for question answering.
Open question answering approach of \cite{Fader:2014:OQA:2623330.2623677} combines multiple structured (Freebase) and unstructured (OpenIE) knowledge bases together by converting them to string-based triples.
User question can be first paraphrased using paraphrasing model learned from WikiAnswers data, then converted to a KB query and certain query rewrite rules can be applied, and all queries are ranked by a machine learning model.

SPOX tuples, proposed in \cite{yahya2013robust}, encode subject-predicate-object triples along with certain keywords, that could be extracted from the same place as RDF triple.
These keywords encode the context of the triple and can be used to match against keywords in the question. The method attempts to parse the question and uses certain relaxations (removing SPARQL triple statements) along with adding question keyphrases as additional triple arguments.
As an extreme case of relaxation authors build a query that return all entities of certain type and use all other question terms to filter and rank the returned list.

However, by applying information extraction to raw text we inevitably lose certain portion of the information due to recall errors, and extracted data is also sometimes erroneous due to precision errors.
In \cite{xu2016enhancing}, authors propose to use textual evidence to do answer filtering in a knowledge base question answering system.
On the first stage with produce a list of answers using traditional information extraction techniques, and then each answer is scored using its Wikipedia page on how well it matches the question. 
Knowledge bases can also be incorporated inside TextQA systems.
Modern KBs contain comprehensive entity types hierarchies, which were utilized in QuASE system of \cite{Sun:2015:ODQ:2736277.2741651} for answer typing.
In addition, QuASE exploited the textual descriptions of entities stored in Freebase knowledge base as answer supportive evidence for candidate scoring.
However, most of the information in a KB is stored as relations between entities, therefore there is a big potential in using all available KB data to improve question answering.

Another great example of a hybrid question answering system is IBM Watson, which is arguably the most important and well-known QA systems ever developed so far.
It was designed to play the Jeopardy TV show\footnote{https://en.wikipedia.org/wiki/Jeopardy!}.
The system combined multiple different approaches, including text-based, relation extraction and knowledge base modules, each of which generated candidate answers, which are then pooled together for ranking and answer selection.
The full architecture of the system is well described in \cite{ferrucci2010building} or in the full special issue of the IBM Journal of Research and Development \cite{ibm_watson_special_issue}.
YodaQA \cite{baudivs2015yodaqa} is an open source implementation of the ideas behind the IBM Watson system.

% The main difference between such systems and the proposed research is that hybrid systems typically use separate pipelines to extract candidates from different sources and only merge the candidate set while ranking.
% I propose to extend the representation of each of the data sources for better candidate generation from the beginning.

% [!!!QALD HYBRID TRACK]

\section{Non-factoid question answering}
\label{sec:rel_work:nonfactoid}

During earlier days of research non-factoid questions received relatively little attention.
TREC QA tasks started to incorporate certain categories of non-factoid questions, such as definition questions, during the last 4 years of the challenge.
One of the first non-factoid question answering system was described in \cite{soricut2006automatic} and was based on web search using chunks extracted from the original question.
The ranking of extracted answer candidates was done using a translation model, which showed better results than n-gram based match score.

The growth of the popularity of community question answering (CQA) websites, such as Yahoo! Answers, Answers.com, \etc, contributed to an increased interest of the community to non-factoid questions.
Some questions on CQA websites are repeated very often and answers can easily be reused to answer new questions, \cite{Liu:2008:USA:1599081.1599144} studies different types of CQA questions and answers and analyzes them with respect to answer re-usability.
A number of methods for similar question retrieval have been proposed \cite{bernhard2009combining,Shtok:2012:LPA:2187836.2187939,duan2008searching,Jeon:2005:FSQ:1099554.1099572}.
WebAP is a dataset for non-factoid answer sentence retrieval, which was developed in \cite{yang2016beyond}.
Experiments conducted in this work demonstrated, that classical retrieval methods doesn't work well for this task, and multiple additional semantic (ESA, entity links) and context (adjacent text) features have been proposed to improve the retrieval quality.

Candidate answer passages ranking problem becomes even more difficult in non-factoid questions answering as systems have to deal with larger piece of text and need to ``understand'' what kind of information is expressed there.
One of the first extensive studies of different features for non-factoid answer ranking can be found in \cite{surdeanu2011learning}, who explored information retrieval scores, translation models, tree kernel and other features using tokens and semantic annotations (dependency tree, semantic role labelling, \etc) of text paragraphs.
Alignment between question and answer terms can serve as a good indicator of their semantic similarity.
Such an alignment can be produced using a machine learning model with a set of features, representing the quality of the match \cite{wang2015faq}.
Alignment and translation models are usually based on term-term similarities, which are often computed from a monolingual alignment corpus.
This data can be very sparse, and to overcome this issue \cite{fried2015higher} proposed higher-order lexical semantic models, which estimates similarity between terms by considering paths of length more than 1 on term-term similarity graph.
An alternative strategy to overcome the sparseness of monolingual alignment corpora is to use the discourse relations of sentences in a text to learn term association models \cite{sharp2015spinning}.

Questions often have some metadata, such as categories on a community question answering website.
This information can be very useful for certain disambiguations, and can be encoded in the answer ranking model \cite{zhou2015learning}.
The structure of the web page, from which the answers are extracted can be very useful as well.
Wikipedia articles have a good structure, and the information encoded there can be extracted in a text-based knowledge base, which can be used for question answering \cite{sondhi2014mining}.
Information extraction methods can also be useful for the more general case of non-factoid question answering.
For example, there is a huge number of online forums, FAQ-pages and social media, that contain question-answer pairs, which can be extracted to build a collection to query when a new question arrives \cite{cong2008finding,Jijkoun:2005:RAF:1099554.1099571,Yang:2009:ISK:1526709.1526735,ding2008using,li2011question}.

Most of the approaches from TREC LiveQA 2015 combined similar question retrieval and web search techniques \cite{ecnucs_liveqa15,savenkov_liveqa15,diwang_liveqa15}.
Answers to similar questions are very effective for answering new questions \cite{savenkov_liveqa15}.
However, we a CQA archive doesn't have any similar questions, we have to fall back to regular web search.
The idea behind the winning system of CMU \cite{diwang_liveqa15} is to represent each answer with a pair of phrases - clue and answer text.
Clue is a phrase that should be similar to the given question, and the passage that follows should be the answer to this question.

\section{User interactions}
\label{sec:rel_work:user}

In this section I will describe the existing research and key differences from my thesis work in the area of interactions between a QA system and human users or experts.

\subsection{Crowdsourcing for Question Answering}
\label{sec:rel_work:user:crowdsourcing}
%-=-=-=-=-=- FROM CROWDSOURCING PAPER

Using the wisdom of a crowd to help users satisfy their information needs has been studied before in the literature.
\cite{bernstein2012direct} explored the use of crowdsourcing for offline preparation of answers to tail search queries.
In this work log mining techniques were used to identify potential question-answer fragment pairs, which were then processed by the crowd to generate the final answer.
This offline procedure allows a search engine to increase the coverage of direct answers to user questions.
In our work, however, the focus is on online question answering, which requires fast responses to the user, who is unlikely to wait more than a minute.
Another related work is targeting a different domain, namely SQL queries.
The CrowdDB system \cite{franklin2011crowddb} is an SQL-like processing system for queries, that cannot be answered by machines only.
In CrowdDB human input is used to collect missing data, perform computationally difficult functions or matching against the query.
In \cite{aydin2014crowdsourcing} authors explored efficient ways to combine human input for multiple choice questions from the ``Who wants to be a millionaire?'' TV show.
In this scenario going with the majority for complex questions isn't effective, and certain answerer confidence weighting schemas can improve the results.  

Using crowdsourcing for relevance judgments has been studied extensively in the information retrieval community, e.g., \cite{Alonso:2008:CRE:1480506.1480508,alonso2011design,grady2010crowdsourcing} to name a few.
The focus in these works is on document relevance, and the quality of crowdsourced judgments.
Whereas in our paper we are investigating the ability of a crowd to quickly assess the quality of the answers in a nearly real-time setting.

Crowdsourcing is usually associated with offline data collection, which requires significant amount of time.
Its application to (near) real-time scenarios poses certain additional challenges.
\cite{bernstein2011crowds} introduced the retainer model for recruiting synchronous crowds for interactive real-time tasks and showed their effectiveness on the best single image and creative generation tasks.
We are planning to build on these ideas and integrate a crowd into a real-time question answering system.
The work of \cite{Lasecki:2013:CCC:2501988.2502057} showed how multiple workers can sit behind a conversational agent named Chorus, where human input is used to propose and vote on responses. 
Another use of a crowd for maintaining a dialog is presented in \cite{Bessho:2012:DSU:2392800.2392841}, who let the crowd handle difficult cases, when a system was not able to automatically retrieve a good response from the database of twitter data.
In this paper, we focus on a single part of the human-computer dialog, i.e. question answering, which requires a system to provide some useful information in a response to the user.


%-=-=-=-=-=-= END FROM CROWDSOURCING PAPER

\subsection{User Assistance}
\label{subsec:rel_work:user:assistance}
%-=-=-=-=-= FROM HINTS PAPER

There has been considerable amount of work on user assistance for general web search and improving user experience with feedback, suggestions and hints.
Results of the study in \cite{xie2009understanding} demonstrate that in 59.5\% of the cases users need help to refine their searches or to construct search statements.
% That confirms findings of \cite{Holscher2000337}.
Individual term (\cite{ruthven2003survey}) or query suggestion (\cite{Bhatia:2011:QSA:2009916.2010023, Cao:2008:CQS:1401890.1401995,Jones:2006:GQS:1135777.1135835}) are among the most popular techniques for helping users to augment their queries.
The study in \cite{Kelly:2009:CQT:1571941.1572006} demonstrated that users prefer query suggestions over term relevance feedback, and that good manually designed suggestions improve retrieval performance.
Query suggestion methods usually use search logs to extract queries that are similar to the query of interest and work better for popular information needs \cite{Bhatia:2011:QSA:2009916.2010023}.

When query or term suggestions are not available, it is still possible to help users by providing potentially useful search hints.
An adaptive tool providing tactical suggestions was presented in \cite{Kriewel2010} and users reported overall satisfaction with its automatic non-intrusive advices.
Modern search engines have many features that are not typically used by an average user, but can be very useful in particular situations as shown in \cite{Moraveji:2011:MIU:2009916.2009966}. The study demonstrated the potential effectiveness and teaching effect of hints.
Different from \cite{Moraveji:2011:MIU:2009916.2009966} in this thesis work I focus on a different type of hints.
Rather than suggesting to use certain advanced search tools, I explore the effectiveness of \textit{strategic} search hints, designed to suggest a strategy a user can adapt to solve a difficult information question.

%-=-=-=-=-= END FROM HINTS PAPER

\subsection{Clarification Questions}
\label{sec:rel_work:user:clarifictions}

%-=-=-=-=-= FROM PAVEL'S CLARIFICATION QUESTIONS RELATED WORK

Early QA studies considered users the sole proactive part asking refining questions and clarifying on system's response \cite{deboni2005}.
QA with a more active system's role was investigated within complex interactive QA (ciQA) TREC track: assessors provided additional information in various forms to live QA systems as a follow-up to initial inquiry; systems produced updated answers upon interactive sessions \cite{trec2007}.
The track outcomes were mixed: interactive phase degraded initial results in most cases; evaluation design was found not quite appropriate for interactive QA.

Kotov and Zhai~\cite{kotov2010} introduced a concept of \textit{question-guided search}, which can be seen as a variant of query suggestion scenario: in response to initial query the user is presented with a list of natural language questions that reflect possible aspects of the information need behind the query.
%Questions are generated based on parsed sentences from  Wikipedia; a ranking model for questions is proposed.
Tang et al.~\cite{tang2011} proposed a method for refinement questions generation consisting of two steps: 1)~refinement terms are extracted from a set of similar questions retrieved from a question archive; 2)~terms are  clustered using a WordNet-like thesaurus, cluster type (such as \textit{location} or \textit{food}) defines the question template to be used.
Sajjad et al.~\cite{sajjad12} described a framework for search over a collection of items with textual descriptions exemplified with xbox avatar assets (appearance features, clothes, and other belongings).
Multiple textual descriptions for each item were gathered via crowdsourcing; attribute--value pairs were extracted subsequently.
In online phase intermediate search results are analyzed and yes/no questions about attributes and values are generated sequentially in order to bisect the result set and finally come to the sought item.
Gangadharaiah and Narayanaswamy~\cite{gangadharaiah2013} elaborated a similar approach to search results refinement through clarification questions.
The authors considered customer support scenario using forum data.
In offline phase noun phrases, attribute--value pairs, and action tuples are extracted from forum collection.
In online phase answers to automatically generated questions help reduce the answer candidates' set.
Despite we are not aimed at building a full-fledged QA system, we follow the same search scenario involving clarification questions proposed in these studies.
The difference is that we seek insights to elaborate on the task in user-generated content.

Kato et al.~\cite{kato2013} investigated clarification questions in context of an enterprise  Q\&A instant messaging in software domain.
Analysis has shown that about one third of all dialogues have clarification requests; 8.2\% of all utterances in the log are related to clarifications. 
The authors developed a question classifier that prompted the asker to provide clarifications in case the request was identified as underspecified. 
%Interestingly, after details were provided the questions were often less likely to be answered.
Different from this work, in my thesis I study different kinds of data, namely CQA archives, and consider a problem of automatic clarification generation.

In spoken dialog systems clarification questions can be aimed at resolving speech recognition uncertainty either on individual words or whole utterances~\cite{stoyanchev2013}.
A cognate line of research aims at building dialog systems that learn from large human conversation logs, e.g. Twitter status--response pairs~\cite{ritter2011}.
However, such open-domain approach is suited rather for entertainment purposes or companionship than information-seeking scenario.

%Our study is also related to the emerging research on mobile intelligent assistant.
%Jiang et al.~\cite{jiang2015} evaluated users' satisfaction with such applications and showed that Web search constitute about 30\% of all interactions, though considered them less `conversational' than device control and chat uses. 

%A large body of work deals with community question answering data, data from Stack Exchange sites in particular.
%For example, Anderson et al.~\cite{anderson2012} showed that long-term value of a question and its answers on Stack Overflow was positively correlated with the number of comments on the answers and the time for highest-score answer to arrive. A comprehensive list of studies that used employed Stack 
%Exchange data is maintained by the community.\footnote{\url{http://meta.stackexchange.com/questions/134495/academic-papers-using-stack-exchange-data}}

%-=-=-=-=-= END FROM PAVEL'S CLARIFICATION QUESTIONS RELATED WORK

% \cite{braunstain2016supporting} retrieves Wikipedia statements that support user answers for opinion questions.

% An interesting approach for knowledge base construction through dialog with the user has been proposed by \cite{hixon2015learning}.


% A number of works have focused on studying and estimating different factors of user satisfaction with question answering systems \cite{ong2009measurement,Liu:2008:PIS:1390334.1390417}


%\section{Summary of Related Work}
%\label{sec:rel_work:summary}
% Not sure I need this and if I do, what to write here...
% Most previous work in ...

\clearpage