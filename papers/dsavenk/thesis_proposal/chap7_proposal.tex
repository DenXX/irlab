% chap7_proposal.tex
%

\mychapter{Summary \& Research Timeline}
\label{chapter:proposal}

\noindent

This chapter summarizes my PhD thesis research proposal, aiming to improve different aspects of question answering.

\section{Research Objectives}
\label{section:proposal:objectives}

The main research objectives of my thesis are the following:

\begin{enumerate}
\item RQ1. How to effectively combine unstructured text and structured knowledge base data to improve factoid question answering?
\item RQ2. How to improve candidate retrieval and answer generation for non-factoid question answering?
\item RQ4. How to use crowdsourcing to improve performance of question answering systems, operating in near real-time?
\item RQ3. How to utilize the dialog between users and a question answering system to improve success rate in solving informational tasks?
\end{enumerate}

\section{Research Plan}
\label{section:proposal:plan}

\subsection{Combining KB and Text Data for Factoid Question Answering (Chapter \ref{chapter:factoid})}
\label{section:proposal:plan:factoid}

My prior research on the topic include a model for relation extraction from question-answer pairs, \eg available on community question answering platform, and incorporating textual resources into a knowledge base question answering system.
The former result allows to increase the amount of factual information one can extract from CQA forums, and therefore help the knowledge base incompleteness problem a little bit more.
The later result shows that unstructured text data can be very helpful for knowledge base question answering, even for a subset of questions, that were designed to be answerable from a KB.

The model I propose to build is based on semantic annotations of entities mentioned in text documents, which essentially creates KB-Text edges, connecting entities to their mentions and vice versa.
These edges can be traversed in both directions, \ie from a passage of text to mentioned KB entities, and from an KB entity to the corresponding mentions using text retrieval methods.
For passages, that mention multiple entities this representation essentially creates an additional edge between these entities.
Unlike existing Open Information Extraction~\cite{Fader:2014:OQA:2623330.2623677,FaderSE11} methods, the proposed approach does not attempt to recover a simple predicate, expressing the relationship, but rather store the whole passage, which can be retrieved using standard information retrieval methods or more recent embedding-based similarity functions.
Such an approach will therefore preserve all the information stored in unstructured format.
Section~\ref{section:factoid:proposal} described the proposed approach in more detail.

\subsubsection{Expected contributions}
\label{section:proposal:plan:factoid:contributions}

\begin{itemize}
\item A novel model for relation extraction from archives of question-answer pairs.

The developed model allows to extract relational triples for KB completion from a novel data source, \ie archives of question-answer pairs, which are available on various community question answering platforms.
The experiments demonstrate, that together with existing sentence-based relation extraction techniques it is now possible to get more information, which eventually benefits the performance of knowledge base question answering and other downstream KB applications.

\item New hybrid KB-Text question answering approach, that improves knowledge base question answering by using information from unstructured text data sources, annotated with KB entity mentions, which essentially introduces a new types of edges into a knowledge graph.

The proposed model has a potential to overcome many of the issues of KB and text question answering, and allow to answer complex factual questions, that could not be answered before.
This research could be useful for future work on integrating different unstructured, semi-structured and structured data sources for joint reasoning in question answering and other applications.

\end{itemize}

\subsubsection{Risks}
\label{section:proposal:plan:factoid:risks}

\begin{itemize}
\item The dataset I propose to derive from CQA data might be hard to build due to potentially low number of questions, that survive the filtering. In case this happens I will have to use one of the other existing datasets, such as TREC QA.
\item Abundant textual information might add a lot of noise to our knowledge graph extended with text-based edges. More connections means more candidates for our search algorithm, and increased level of noise might be hard to deal with in this scenario, which could actually drop the performance compared to clean, but incomplete knowledge base only model.
\end{itemize}


\subsection{Answer Summarization for Non-factoid Question Answering (Chapter~\ref{chapter:non-factoid})}
\label{section:proposal:plan:non-factoid}

The system I built to participate in TREC LiveQA shared tasks provided a good baseline of todays capabilities of automatic question answering systems for complex user informational needs, which are often solved using community question answering platforms.
One of the challenges of such QA pipelines is that a single passage, extracted from somewhere on the web, might not be enough to answer a user question.
In Section~\ref{section:non-factoid:proposal} I proposed to develop answer summarization techniques, inspired by the recent advances in deep learning for text summarization \cite{rush-chopra-weston:2015:EMNLP,chopraabstractive16} and generation \cite{karpathy2015deep}.
More particularly, I'm planning to test both extractive and abstractive text summarization approaches on the task of generating responses to non-factoid questions.

\subsubsection{Expected contributions}
\label{section:proposal:plan:non-factoid:contributions}

\begin{itemize}
\item An open source non-factoid question answering system, designed to participate in TREC LiveQA shared task, and which can be used as a baseline in various experiments in improving different aspects of the QA process.

The system I developed was ranked top-7 in TREC LiveQA 2015 shared task (the results of the 2016 track aren't yet available).

\item A novel answer summarization module for non-factoid question answering system.
Unlike some prior work, which focused on summarizing answers posted by different users on CQA platforms, the model I propose to develop will operate inside a real question answering system, which means it will have to deal with some additional challenges, such as higher rate of irrelevant passages.
This model will be useful as a first step towards more sophisticated answer distillation techniques~\cite{mitra2016distillation}.

\end{itemize}

\subsubsection{Risks}
\label{section:proposal:plan:non-factoid:risks}

\begin{itemize}
\item The amount of training data available for the task isn't that big compared to some other datasets, used to train deep learning models. There is a risk, that it won't be enough to find the optimal parameters for the model and beat some traditional techniques
\end{itemize}

\subsection{Crowdsourcing for near Real-time Question Answering (Chapter~\ref{chapter:crowdsourcing})}
\label{section:proposal:plan:crowdsourcing}

The developed near real-time QA system showed significant answer performance improvements compared to a fully automatic one.
As a future research I propose to optimize the costs of this approach.
To make our system more scalable and cut expenses associated with the crowdsourcing module, I propose to incorporate a model to predict the expected performance of the answer, generated by the automated system.
This score can be used to prioritize tasks when the load is high, and even skip some tasks altogether, if the expected quality of automatically generated answer is high.
This piece of work will be included in my thesis if time permits.

\subsubsection{Expected contributions}
\label{section:proposal:plan:crowdsourcing:contributions}

\begin{itemize}
\item New method for answer collection and rating using crowdsourcing for a near real-time question answering system.

I believe that this piece of work could be very useful for future research on the interaction between automatic intelligent systems and human experts, which could be an essential part of modern intelligent assistants.
Existing systems works great for a subset of user tasks, but encounter certain problems for a different subset.
To provide a smooth user experience it might be crucial to have a fall back option of human experts, who can help systems to overcome the challenge.

\item A novel hybrid question answering system, that incorporates crowdsourcing, but still operates in near real-time, \ie providing a response within one minute after the question is posted.
This is much faster than on existing community question answering platforms, where a quarter of the questions are left completely unanswered.
\end{itemize}

\subsubsection{Risks}
\label{section:proposal:plan:crowdsourcing:risks}

\begin{itemize}
\item High overall costs of the developed crowdsourcing module.
In the current version of our hybrid question answering system, developed for TREC LiveQA 2016 the cost of a single question turned out to be \$0.81, which is quite a lot.
The proposed research targets the problem of reducing these costs by applying crowdsourcing selectively, but there is a risk that the cost of getting a significant quality improvement will still be high to make our system practically useful.
\end{itemize}

\subsection{User Interactions with Question Answering Systems (Chapter~\ref{chapter:users})}
\label{section:proposal:plan:users}

In my previous research I studied strategic search hints, that a search system can give to the user in response to a complex informational tasks.
If a user is not satisfied with the retrieved results, she can try to follow the suggestions and split the task into simpler pieces, which should increase the success rate.
The results showed higher success rates for users who received helpful suggestions, however, more general hints, that are hard to implement could be detrimental.
However, such an approach put all the heavy lifting of formulating the questions on the user.
In the future research for my thesis I propose to develop methods for positive and negative relevance feedback for question answering in a dialog scenario.
After a system returns an answer, a user might respond with certain feedback, whether positive (\eg ``give me more information on the topic'') or negative (\eg ``this answer is bad'').
The system needs to take this feedback into account and come up with a better answer.
To implement this functionality I propose to extend a set of features representing each answer candidates with features, measuring similarities and dissimilarities between the candidate and provided positive and negative feedback answers.
More details on the architecture of the proposed approach are given in Section~\ref{section:users:proposal:method}.

\subsubsection{Expected contributions}
\label{section:proposal:plan:users:contributions}

\begin{itemize}
\item A study of the effect of strategic search hints on the user experience and success rate for complex informational tasks.

The results of this work suggest, that well designed search hints can have a positive effect on users struggling with a complex search task.
These observations provide some insights for future research in user assistance for complex information needs.

\item A novel method for positive and negative relevance feedback in a question answering dialog scenario. 
I believe that incorporating user feedback is a first step towards a richer dialog between a computer system and a user.
In future, it's possible to extend a set of dialog actions and allow system ask clarification questions or confirm certain pieces of information.
\end{itemize}

\subsubsection{Risks}
\label{section:proposal:plan:users:risks}

\begin{itemize}
\item Negative relevance feedback is usually harder to implement that positive feedback~\cite{wang2008study}.
Negative feedback tells the system some information on what user didn't like.
However, the space of irrelevant information is much larger than relevant, \ie another candidate answer might be different from the one that received negative feedback, but still doesn't provide any useful information.
Therefore, if in our scenario bad questions turns out to be quite diverse, negative feedback might not work efficiently, and the overall user experience would suffer.
\end{itemize}

\section{Research Timeline}
\label{section:proposal:timeline}

A tentative timeline for the work that needs to be done is shown below:

\begin{itemize}
\item Joint model for question answering over KB and text (Section~\ref{section:factoid:problem}): \textit{8/2016 - 9/2016}
	\begin{enumerate}
	\item collect and index textual data about entity mentions from ClueWeb12 dataset
	\item derive a factoid question answering dataset from Yahoo!Answers WebScope collection
	\item build a joint KB and text question answering system
	\item test it on existing (TREC QA) as well is derived datasets and compare the results with the baselines
	\end{enumerate}
	
\item Answer summarization for non-factoid question answering (Section~\ref{section:non-factoid:proposal}): \textit{9/2016 - 10/2016}
	\begin{enumerate}
	\item build a model for extractive summarization for non-factoid question answering
	\item if time permits, build a model for abstractive summarization for non-factoid question answering
	\item test the model on Yahoo! Answers datasets from~\cite{omari2016novelty,tomasoni2010metadata}
	\item integrate the model into my TREC LiveQA system, test it on a set of questions and manually judge the quality compared to a single returned answer.
	\end{enumerate}

\item Relevance feedback for dialog-based question answering (Section~\ref{section:users:proposal}): \textit{9/2016 - 11/2016}
	\begin{itemize}
	\item implement features to measure similarity between an answer candidate and negative and positive feedback answers
	\item train candidate answer ranking model, that incorporates relevance feedback if available
	\item test the model on the data from TREC LiveQA 2016
	\item conduct a small user study to see how the system behaves in a real scenario and whether or not it improves the user experience
	\end{itemize}

\item If time permits, optimizing crowdsourcing for near real-time question answering (Section~\ref{section:crowdsourcing:proposal}): \textit{11/2016}
\begin{enumerate}
	\item build a model to predict the expected quality of automatically generated answer candidate
	\item develop a task scheduling system based on priorities estimated in step 1, number of workers available and the current load of the system.
	\item test the system by simulating arrivals of the question using different models
\end{enumerate}


\item Thesis writing: \textit{10/2016 - 11/2016}
\item Thesis defense: \textit{12/2016}
\end{itemize}

\section{Summary}
\label{section:proposal:summary}

In my thesis I proposed several pieces of work towards improving user satisfaction with question answering systems for factoid as well as non-factoid information needs.
The proposed research spans multiple directions: from improving the data sources for factoid question answering, better answer representation by summarizing available pieces of information, to engaging the human input, either from a crowd of external workers or the user herself in a form of feedback.
I believe, that the results of the proposed research directions will be useful for the developing field of intelligent personal assistants and chat bots, as question answering is arguably one of their most important applications.
