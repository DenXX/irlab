% chap1.tex
%
% First chapter file is different from others
%
\mychapter{Introduction}
\label{chapter:intro}

\pagenumbering{arabic}
\setcounter{page}{1}
\pagestyle{myheadings}

%Uncomment to switch spacing.
%\baselineskip=5px

\newtheorem{definition}{Definition}
\newtheorem{proposition}{Proposition}


\section{Motivation}

It has long been a dream to communicate with a computer as one might with another human being using natural language speech and text.
Nowadays, we are coming closer to this dream, as natural language interfaces become increasingly popular.
Our phones are already reasonably good at recognizing speech, and personal assistants, such as Apple Siri, Google Now, Microsoft Cortana, Amazon Alexa, etc., help us with everyday tasks and answer some of our questions.
Chat bots are arguably considered ``the next big thing'', and a number of startups developing this kind of technology has emerged in Silicon Valley and around the world\footnote{http://time.com/4194063/chatbots-facebook-messenger-kik-wechat/}.

Question answering is one of the major components of such personal assistants.
Existing techniques already allow users to get direct answers to some of their questions.
However, by some estimates\footnote{https://www.stonetemple.com/the-growth-of-rich-answers-in-googles-search-results/} for $\sim$ 70\% of more complex questions users still have to dig into the ``10 blue links'' and extract or synthesize answers from information buried within the retrieved documents.
In order to make a shift towards more intelligent personal assistants this gap needs to be closed.
Therefore, in my thesis I focus on helping users get answers to their questions by improving question answering methods and the ways a system interact with its users.

User questions vary in many different aspects, each of which has its own set of challenges.
It's common to divide questions into \textit{factoid} and \textit{non-factoid}.
Factoid questions are inquiring about certain facts and can be answered by a short phrase (or list), \ie entity name, date or number.
An example of a factoid question is ``\textit{What book did John Steinbeck wrote about the people in the dust bowl?}'' (answer: ``\textit{The Grapes of Wrath}'').
Of course, there is a variety of questions, that do not fall into this group, \eg how-to and why questions, recommendation and opinion questions, \etc.
The literature usually refers to all these questions by ``non-factoid questions'' umbrella term.
Most of the research in automatic question answering focused on factoid questions \cite{voorhees2001trec}, and recently more and more works target often more complex non-factoid category \cite{overviewliveqa15}.
These types of questions provide quite distinct set of challenges and methods applied to them are often quite different, therefore in my thesis I will first study factoid QA and then propose some ideas to improve non-factoid QA.

Automated question answering systems use various data sources to generate answers to user questions.
By their nature, data sources can be classified into \textit{unstructured} (\eg raw natural language text), \textit{semi-structured} (\eg tables) and \textit{structured} (\eg knowledge bases).
Each of these types of data has certain advantages and limitations (Table \ref{table:data_procons}).
There are a number of methods designed for question answering using text collections, knowledge bases or archives of question-answer (QnA) pairs.
Most of the developed systems use either a single source of data, or combine multiple independent pipelines, each of which operates over a separate data source.
Motivated by the fact that advantages and disadvantages of structured and unstructured data sources complement each other, In my thesis I propose to study methods of combining different data sources for joint reasoning for factoid and non-factoid questions.

\begin{table}
\centering
\caption{Pros and cons of structured and unstructured data sources for factoid and non-factoid question answering}
\begin{tabular}{| l | p{6cm} | p{6cm} |}
\hline
 & unstructured data & structured data \\
\hline
factoid questions & \multicolumn{1}{|c|}{Text} & \multicolumn{1}{|c|}{Knowledge Bases} \\
 & + easy to match against question text & + aggregate all the information about entities\\
 & + cover a variety of different information types & allow complex queries over this data using special languages (e.g. SPARQL) \\
 & - each text phrase encodes a limited amount of information about mentioned entities & - hard to translate natural language questions into special query languages \\
&  & - KBs are incomplete (missing entities, facts and properties) \\
\hline
non-factoid questions & \multicolumn{1}{|c|}{Text} & \multicolumn{1}{|c|}{Question-Answer pairs} \\
 & + contain relevant information to a big chunk of user needs & + easy to find a relevant answer by matching the corresponding questions \\
 & - hard to extract semantic meaning of a paragraph to match against the question (lexical gap) & - cover a smaller subset of user information needs \\
\hline
\end{tabular}
\label{table:data_procons}
\end{table}

Two major paradigms for factoid question answering are knowledge base question answering (KBQA) and text-based question answer (TextQA).
Information contained in a huge volume of text data on the web can be relatively easily queried using terms and phrases from the original question in order to retrieve sentences that might contain the answer.
However, each sentence encode very limited amount of information about mentioned entities and aggregating it over unstructured data is quite problematic.
On the other hand, modern large scale knowledge bases, such as Freebase \cite{Bollacker:2008:FCC:1376616.1376746}, dbPedia \cite{auer2007dbpedia}, YAGO \cite{yago3}, WikiData \cite{vrandevcic2014wikidata}, aggregate information about millions of entities into a graph of [subject, predicate, object] RDF triples.
The problem with KBs is that they are inherently incomplete and miss a lot of entities, facts and predicates.
In addition, triple data representation format complicates retrieval of KB concepts relevant to question phrases.
The focus of the proposed research in factoid question answering lies on the idea of combining structured KB and unstructured text data, which can help a QA system to overcome these drawbacks.


% THIS PIECE IS GOOD, BUT IT DUPLICATES SOMETHING I HAVE ALREADY SAID. KEEPING IT JUST IN CASE...
% Billions of documents on the web contain all kinds of knowledge about the world, which can be retrieved to answer user questions.
%However, each individual statement includes a very limited amount of information about mentioned entities.
%On the other side, modern open domain large scale knowledge bases, such as dbPedia\footnote{http://wiki.dbpedia.org/}, YAGO\cite{yago3}, Freebase\footnote{http://www.freebase.com}, WikiData\footnote{https://www.wikidata.org/}, etc., contain millions of entities and facts about them, and are quite effective in answering some of the user questions.
%However, knowledge bases have their own disadvantages:
%\begin{itemize}
%\item knowledge bases are inherently incomplete \cite{Dong:2014:KVW:2623330.2623623}, even the largest existing resources miss a lot of entities, facts and properties, that might be of interest to some users.
%\item it's quite challenging to translate a natural language question into a structured language, such as SPARQL, to query a knowledge base \cite{BerantCFL13:sempre}.
%\end{itemize}

The main challenge in non-factoid question answering lies in the diversity of question and answer types.
One of the most effective strategies is to reuse answers to previously asked questions, which could be found, for example, in CQA archives \cite{Shtok:2012:LPA:2187836.2187939}.
Unfortunately, it's not always possible to find a similar question, that has already been answered, because many information needs are unique in general or in details.
Alternative strategies include ranking text passages extracted from retrieved web documents.
One of the main challenges of this approach is estimating semantic similarity between the question and an answer candidate \cite{soricut2006automatic}.
Therefore, one would benefit from knowing what kind of questions could a paragraph of text answer.
This information can often be inferred from the structure of a web page, e.g. forums, FAQ pages, or estimated using title, subtitle and other page elements.
Therefore, one of the questions I'm going to focus in my thesis is how to effectively use the structure of web page to predict whether an extracted passage of text answer the given question.

However, ranking isn't the only important part of the question answering pipeline.
A system can only rank and return a good answer if it was able to retrieve relevant information from a collection.
Non-factoid questions, especially those that people post on CQA websites are often long, which makes it problematic to use directly as search queries.
Previous research has studied certain question transformation strategies \cite{AgichteinLG01,brill_askmsr,lin2003question}, however the focus was on shorter factoid questions.
In my thesis I would like to focus on the problem of query generation for non-factoid questions using some recent advances in deep learning.
Another promising direction of research, which I'm going to explore in my thesis, is answer generation, \ie by summarizing the information a system could retrieve.
Different answer candidates might by complimentary to each other, answer different parts of the question or provide complimentary opinions on the subject.

Unfortunately, no matter how good a QA system is, there will always be cases, when it is impossible to return a satisfactory answer to user's question, \eg existing data sources might not contain necessary information or the question may simply be ambiguous or poorly worded.
In the former situation a QA system can appeal to an alternative external data source, \eg other people via crowdsourcing, while in the later scenario a system should probably reply to the user with some clarification question or give some kind of feedback on how she could solve her information need.


\section{Research Questions}

Research questions I proposed addressed in my thesis are the following:

\begin{enumerate}
\item RQ1. How to effectively combine unstructured text and structured knowledge base data to improve factoid question answering?
% THESE ARE SUBQUESTIONS, I DON'T THINK I NEED TO STATE THEM HERE.
% \begin{enumerate}
%  \item What types of questions can be answered using text, KB or a combination of both?
%  \item How does semantic annotation of unstructured data compare to information extraction for question answering?
%     (information extraction for KB construction vs open information extraction vs unstructured data annotation)
%  \item How does a combination of structured and unstructured data sources improve each of the main QA system components: question analysis, candidate generation, evidence extraction and answer selection?
%\end{enumerate}
\item RQ2. What kind of information about a web page can help scoring a passage extracted from it as a possible answer to the given question?
\item RQ3. How to build question and answer summarization models to improve candidate retrieval and answer generation for non-factoid question answering?
\item RQ4. How we can improve user experience with question answering systems for complex informational tasks?
\end{enumerate}


\section{Research Plan}

\subsection{Combining KB and Text Data for Factoid Question Answering (Chapter \ref{chapter:factoid})}
\label{sec:plan1}

The goal is to study a problem of using multiple structured KB and unstructured data together to improve factoid question answering.
Two major issues with KBQA is knowledge base incompleteness and complexity of translating natural language question into a structured query.
Text documents on the other hand are easier to match against the question, contain more information than a typical knowledge base, but aggregating information across multiple statements and documents is complicated.

One way to improve the situation with knowledge base incompleteness is to extract missing information from other data sources, \eg \cite{Cafarella:2008:WEP:1453856.1453916,Cafarella:2009:WES:1519103.1519112,Dong:2014:KVW:2623330.2623623,Etzioni:2008:OIE:1409360.1409378,Gupta:2014:BOS:2732286.2732288,kushmerick1997wrapper}.
I propose to explore one additional data source, that wasn't used for relation extraction before, namely question-answer pairs.
Section \ref{sec:relation_extraction} will describe our experiments and results in utilizing this data to improve knowledge base coverage.
Unfortunately, relation extraction isn't perfect either and there are both precision and recall losses.
Therefore, I propose to explore semantic annotation of entity mentions as a way to bridge the gap between KB entity graph and text documents.
Such representation will allow us to do simple string matching on text documents and at the same time explore the knowledge about the mentioned entities in KB and vice versa. Section \ref{sec:text+kb} describes the approach in more detail.

\subsection{Using Web Page Information to Improve Passage Ranking in Non-factoid Question Answering (Chapter \ref{sec:non-factoid:architecture:page-structure})}
\label{sec:plan2}

To answer RQ2 I'm planning to study what kind of information from web pages can be useful to predict whether a passage of text answer the given questions.
First, I'll derive a dataset of questions from TREC LiveQA'15 with passages, labeled by TREC assessors, and extract the corresponding web pages.
This allows us to set the problem as passage ranking problem using a set of passage and web page context features.
I will design a set of features representing a passage and some key elements from the web page and train an answer ranking model.
The feature ablation experiments will reveal the relative importance of different model components.

\subsection{Question and Answer Summarization for Non-factoid Question Answering (Chapter \ref{sec:non-factoid:architecture:analysis})}
\label{sec:plan3}

The goal is to develop models for question summarization, which should improve candidate answer retrieval performance, and answer summarization to generate the final response of the system.
The plan is to explore recent advances in deep learning for text summarization \cite{rush-chopra-weston:2015:EMNLP} and generation \cite{karpathy2015deep} and apply these techniques for the above mentioned problems.
The effectiveness of these models will be evaluated using the data from TREC LiveQA'15 and tested inside this year challenge model.


\subsection{Interaction between a QA system and humans (Chapter \ref{chapter:users})}
\label{sec:plan4}

In my thesis I'm planning to consider three different types of interactions between a QA system and humans: crowdsourcing, providing users with strategic hints to help them structure their search process, and clarification questions, which a QA system can ask users to resolve certain ambiguities.

For crowdsourcing, I'm going to study how a QA system can leverage a pool of human workers to crowdsource some data, which can help it answer certain difficult questions (Section \ref{sec:non-factoid:crowd}).
Automated QA sytems operate in near real-time, which poses certain challenges for crowdsourcing.
First, I'll study if it is possible to get useful data from crowd workers under a certain time limit, and then implement an almost real-time crowdsourcing system to help an automated system answer questions from TREC LiveQA 2016 shared task.

Strategic search hints are certain suggestions, which an automated system can provide to a user to structure her search task, formulate easier questions that a system can tackle.
I will study how a user react to different types of hints and how the hints affect the overall task success rate (Section \ref{sec:user:hints}).

Finally, for questions that an automated system is unable to understand it make sense to come back to the user with some clarification questions rather than a totally useless answer.
In Section \ref{sec:user:clarification} I study what kind of clarification questions real users ask, and how a system can generate a certain frequent subset of them automatically.


\subsection{Research Timeline}

\noindent
A tentative timeline for the work that needs to be done is shown below:

\begin{itemize}
\item Completing the work proposed in Sections \ref{sec:plan2} and \ref{sec:plan3} (4/2016 - 5/2016): develop individual components for question analysis, answer candidate retrieval and answer generation and integrate them into a system to participate in TREC LiveQA'16.
\item Completing the work proposed in Section \ref{sec:plan1} (6/2016 - 7/2016): Develop a model to use annotation of entity mentions for factoid question answering and compare it against existing techniques. I'm also planning to develop a new QA dataset, which will include more diverse and realistic set of questions than existing KBQA datasets and larger than available TREC QA datasets.
\item Completing the work proposed in Section \ref{sec:plan4} (4/2016 - 7/2016): Integrate a crowdsourcing module into my TREC LiveQA'16 system as one of the options, develop a model to predict ambiguous questions on a CQA website and propose certain clarification questions.
\item Thesis writing (08/2016 - 09/2016) 
\item Thesis defense (10/2016)
\end{itemize}


\section{Contributions and Implications}

The key contributions of the proposed research are:

% ADD HOW EACH OF THE POINTS COULD BE USEFUL FOR THE FIELD

\begin{itemize}
\item A novel model for relation extraction from archives of question-answer pairs
\item New hybrid KB-Text question answering approach, that improves knowledge base question answering by using information from unstructured text data sources, annotated with KB entity mentions, which essentially introduces a new types of edges into a knowledge graph
% that operates by searching an entity graph, built from both facts from a KB and links between KB entities and text fragments mentioning these entities
\item New dataset for entity-centric factoid question-answering built from an archive of CQA question-answer pairs
\item A non-factoid question-answering system, that incorporates novel question and answer summarization components, as well as novel candidate answer ranking features, based on the information extracted from the structure of the source web document
\item New method for answer collection and rating using crowdsourcing for a near real-time question answering system
\item a study of the effect of strategic search hints on the user experience and success rate for complex informational tasks
\item A novel model for detecting ambiguous questions and formulating clarifications
\end{itemize}
