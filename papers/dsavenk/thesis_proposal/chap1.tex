% chap1.tex
%
% First chapter file is different from others
%
\mychapter{Introduction}
\label{chapter:intro}

\pagenumbering{arabic}
\setcounter{page}{1}
\pagestyle{myheadings}

%Uncomment to switch spacing.
%\baselineskip=5px

\newtheorem{definition}{Definition}
\newtheorem{proposition}{Proposition}


\section{Motivation}

It has long been a dream to communicate with a computer as with another human being using natural language speech and text.
Nowadays, we are coming closer to this dream as natural language interfaces become more and more popular.
Our phones are already reasonably good in recognizing speech, and personal assistants, such as Apple Siri, Google Now, Microsoft Cortana, Amazon Alexa, \etc, help us with everyday tasks and answer some of our questions.
Chat bots are arguably considered ``the next big thing'', and a number startups developing this kind of technologies has emerged in Silicon Valley and around the world\footnote{http://time.com/4194063/chatbots-facebook-messenger-kik-wechat/}.

Questions are a natural way of communicating ones information needs.
Users of modern search engines are already used to receiving an answer panel in response to some of their questions \footnote{https://www.stonetemple.com/rich-answers-in-search/}.
However, there is still a big number of questions, for which users have to click on 10 blue links and mine the answer from the huge amount of information contained in retrieved documents.
Therefore, in my thesis I focus on helping users get answers to their questions by improving question answering methods and interactions between a human and a computer system.

It's common to divide questions into \textit{factoid} and \textit{non-factoid}.
Factoid questions are inquiring about certain facts and can be answered with a short phrase (or list), \ie entity name, date or number.
An example of a factoid question is ``\textit{What book did John Steinbeck wrote about the people in the dust bowl?}'' (answer: ``\textit{The Grapes of Wrath}'').
Besides this type of questions, people often ask for recommendations, opinion, instructions, definitions, reason \etc.
These questions are typically combined under an umbrella of non-factoid questions.

There are multiple different types of information sources, that various QA systems use to find the answers to user questions.
These data sources can be classified into unstructured (\eg raw natural language text), semi-structured (\eg tables) and structured (\eg knowledge bases).
Each of these types has certain advantages and limitations (Table \ref{table:data_procons}).

\begin{table*}
\centering
\caption{Pros and cons of structured and unstructured data sources for factoid and non-factoid question answering}
\begin{tabular}{| l | p{6cm} | p{6cm} |}
\hline
 & unstructured data & structured data \\
\hline
factoid questions & \multicolumn{1}{|c|}{Text} & \multicolumn{1}{|c|}{Knowledge Bases} \\
 & + easy to match against question text & + aggregate all the information about entities\\
 & + cover a variety of different information types & allow complex queries over this data using special languages (e.g. SPARQL) \\
 & - each text phrase encodes a limited amount of information about mentioned entities & - hard to translate natural language questions into special query languages \\
&  & - KBs are incomplete (missing entities, facts and properties) \\
\hline
non-factoid questions & \multicolumn{1}{|c|}{Text} & \multicolumn{1}{|c|}{Question-Answer pairs} \\
 & + contain relevant information to a big chunk of user needs & + easy to find a relevant answer by matching the corresponding questions \\
 & - hard to extract semantic meaning of a paragraph to match against the question (lexical gap) & - cover a smaller subset of user information needs \\
\hline
\end{tabular}
\label{table:data_procons}
\end{table*}

Two major paradigms for factoid question answering are knowledge base question answering (KBQA) and text-based question answer (TextQA).
Information contained in a huge volume of text data on the web can be relatively easily queried using terms and phrases from the original question in order to retrieve sentences that might contain the answer.
However, each sentence encode very limited amount of information about mentioned entities and aggregating information over unstructured data is quite problematic.
On the other hand, modern large scale knowledge bases, such as Freebase \cite{Bollacker:2008:FCC:1376616.1376746}, dbPedia \cite{auer2007dbpedia}, YAGO \cite{yago3}, WikiData \cite{vrandevcic2014wikidata}, aggregate information about millions of entities into a graph of [subject, predicate, object] RDF triples.
The problem with KBs is that they are inherently incomplete and miss a lot of entities, facts and predicates.
In addition, triple data representation format complicates retrieval of KB concepts relevant to question phrases.
The focus of the proposed research in factoid question answering lies on the idea of combining structured KB and unstructured text data, which can help a QA system to overcome these drawbacks.
One way to improve the situation with knowledge base incompleteness is to extract missing information from other data sources, \eg \cite{Cafarella:2008:WEP:1453856.1453916,Cafarella:2009:WES:1519103.1519112,Dong:2014:KVW:2623330.2623623,Etzioni:2008:OIE:1409360.1409378,Gupta:2014:BOS:2732286.2732288,kushmerick1997wrapper}.
In my thesis I focus on one particular data source, that didn't receive enough attention in the relation extraction literature, namely question-answer pairs.
Section \ref{sec:relation_extraction} will describe our experiments and results in utilizing this data to improve knowledge base coverage.
Unfortunately, relation extraction isn't perfect either and there are both precision and recall losses.
Alternatively, in my thesis I propose a new hybrid approach to question answering, which leverages a combination of text and knowledge base data to improve every stage of question answering process.
More specifically, I propose to use semantic annotation of KB entity mentions in text documents as a bridge between data sources (Section \ref{sec:text+kb}).

% THIS PIECE IS GOOD, BUT IT DUPLICATES SOMETHING I HAVE ALREADY SAID. KEEPING IT JUST IN CASE...
% Billions of documents on the web contain all kinds of knowledge about the world, which can be retrieved to answer user questions.
%However, each individual statement includes a very limited amount of information about mentioned entities.
%On the other side, modern open domain large scale knowledge bases, such as dbPedia\footnote{http://wiki.dbpedia.org/}, YAGO\cite{yago3}, Freebase\footnote{http://www.freebase.com}, WikiData\footnote{https://www.wikidata.org/}, etc., contain millions of entities and facts about them, and are quite effective in answering some of the user questions.
%However, knowledge bases have their own disadvantages:
%\begin{itemize}
%\item knowledge bases are inherently incomplete \cite{Dong:2014:KVW:2623330.2623623}, even the largest existing resources miss a lot of entities, facts and properties, that might be of interest to some users.
%\item it's quite challenging to translate a natural language question into a structured language, such as SPARQL, to query a knowledge base \cite{BerantCFL13:sempre}.
%\end{itemize}

The main challenge in non-factoid question answering lies in the diversity of question and answer types.
One of the most effective strategies is to reuse answers to previously asked questions, which could be found, for example, in CQA archives \cite{Shtok:2012:LPA:2187836.2187939}.
Unfortunately, it's not always possible to find a similar question, that has already been answered.
Many information needs are unique or contain unique details, which makes it impossible to reuse old answers.
Alternative strategies include ranking text passages extracted from retrieved web documents.
One of the main challenges of this approach is estimating semantic similarity between the question and an answer candidate \cite{soricut2006automatic}.
Therefore, one would benefit from knowing what kind of questions could a paragraph of text answer.
This information can often be inferred from the structure of a web page, e.g. forums, FAQ pages, or estimated using title, subtitle and other page elements.
Therefore, one of the questions I'm going to focus in my thesis is how to effectively use the structure of web page to predict whether an extracted passage of text answer the given question.

However, ranking isn't the only important part of the question answering pipeline.
A system can only rank and return a good answer if it was able to retrieve relevant information from a collection.
Non-factoid questions, especially those that people post on CQA websites are often long, which makes it problematic to use directly as search queries.
Previous research has studied certain question transformation strategies \cite{AgichteinLG01,brill_askmsr,lin2003question}, however the focus was on shorter factoid questions.
In my thesis I would like to focus on the problem of query generation for non-factoid questions using some recent advances in deep learning.
Another promising direction of research, which I'm going to explore in my thesis, is answer generation, \ie by summarizing the information a system could retrieve.
Different answer candidates might by complimentary to each other, answer different parts of the question or have different opinions on the subject.

NEED A PARAGRAPH ABOUT USERS AND QA.

\section{Research Questions}

Research questions I proposed addressed in my thesis are the following:

\begin{enumerate}
\item How to effectively combine unstructured text and structured knowledge base data to improve factoid question answering?
\begin{enumerate}
  \item What types of questions can be answered using text, KB or a combination of both?
  \item How does semantic annotation of unstructured data compare to information extraction for question answering?
     (information extraction for KB construction vs open information extraction vs unstructured data annotation)
  \item How does a combination of structured and unstructured data sources improve each of the main QA system components: question analysis, candidate generation, evidence extraction and answer selection?
\end{enumerate}
\item How to improve question understanding and answer generation for non-factoid question answering using recent advances in deep learning?
\item What kind of information about the structure of a web page can help to score an extracted passage as a candidate answer to the given question?
\item How can we improve user success with question answering by providing them with search strategy hints, clarifications or by employing the crowd?
\end{enumerate}


\section{Research Plan}

\subsection{Step 1 (Chapter \ref{chapter:factoid})}
\label{sec:plan1}

\subsection{Step 2 (Chapter \ref{chapter:non-factoid})}
\label{sec:plan2}

\subsection{Step 3 (Chapter \ref{chapter:users})}
\label{sec:plan3}

\subsection{Research Timeline}

% \noindent A tentative timeline for the work that needs to be done is shown below:

%\begin{itemize}
%\item Completing the work proposed in Section \ref{sec:plan2} (10/2013 - 11/2013): For detecting implicit question intent in web search, build a better evaluation set, develop more features for the classifier and compare with more baselines.
%\item Completing the work proposed in Section \ref{sec:plan3} (12/2013 - 04/2014): For improving question search for queries with question intent, develop models that build better query model and learn better word-to-word translation relationships based on the query-question click data and question category information.
%\item \textbf{If time permits}, extending the work proposed in Section \ref{sec:plan3} (03/2014 - 04/2014): Extend the methods developed above for question search to achieve better query expansion and results ranking for general web search.
%\item Thesis writing (05/2014 - 08/2014) 
%\item Thesis defense (08/2014)
%\end{itemize}


\section{Contributions and Implications}

The key contributions of the proposed research are:
1. New hybrid KB-text question answering algorithm, that is based on graph search, which includes both KB links as well as text search edges to follow.
2. New labelled dataset for question answering (???)
3. New features for ranking answer candidates ???
