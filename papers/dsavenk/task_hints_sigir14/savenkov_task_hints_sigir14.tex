% This is "sig-alternate.tex" V2.0 May 2012
% This file should be compiled with V2.5 of "sig-alternate.cls" May 2012
%
% This example file demonstrates the use of the 'sig-alternate.cls'
% V2.5 LaTeX2e document class file. It is for those submitting
% articles to ACM Conference Proceedings WHO DO NOT WISH TO
% STRICTLY ADHERE TO THE SIGS (PUBS-BOARD-ENDORSED) STYLE.
% The 'sig-alternate.cls' file will produce a similar-looking,
% albeit, 'tighter' paper resulting in, invariably, fewer pages.
%
% ----------------------------------------------------------------------------------------------------------------
% This .tex file (and associated .cls V2.5) produces:
%       1) The Permission Statement
%       2) The Conference (location) Info information
%       3) The Copyright Line with ACM data
%       4) NO page numbers
%
% as against the acm_proc_article-sp.cls file which
% DOES NOT produce 1) thru' 3) above.
%
% Using 'sig-alternate.cls' you have control, however, from within
% the source .tex file, over both the CopyrightYear
% (defaulted to 200X) and the ACM Copyright Data
% (defaulted to X-XXXXX-XX-X/XX/XX).
% e.g.
% \CopyrightYear{2007} will cause 2007 to appear in the copyright line.
% \crdata{0-12345-67-8/90/12} will cause 0-12345-67-8/90/12 to appear in the copyright line.
%
% ---------------------------------------------------------------------------------------------------------------
% This .tex source is an example which *does* use
% the .bib file (from which the .bbl file % is produced).
% REMEMBER HOWEVER: After having produced the .bbl file,
% and prior to final submission, you *NEED* to 'insert'
% your .bbl file into your source .tex file so as to provide
% ONE 'self-contained' source file.
%
% ================= IF YOU HAVE QUESTIONS =======================
% Questions regarding the SIGS styles, SIGS policies and
% procedures, Conferences etc. should be sent to
% Adrienne Griscti (griscti@acm.org)
%
% Technical questions _only_ to
% Gerald Murray (murray@hq.acm.org)
% ===============================================================
%
% For tracking purposes - this is V2.0 - May 2012

\documentclass{sig-alternate}

\usepackage[style=base]{caption}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{xcolor}
\newcommand\todo[1]{\textcolor{red}{#1}}
\renewcommand{\arraystretch}{1.5}

\begin{document}
%
% --- Author Metadata here ---
\conferenceinfo{SIGIR}{'14 Gold Coast, Australia}
%\CopyrightYear{2007} % Allows default copyright year (20XX) to be over-ridden - IF NEED BE.
%\crdata{0-12345-67-8/90/01}  % Allows default copyright data (0-89791-88-6/97/05) to be over-ridden - IF NEED BE.
% --- End of Author Metadata ---

\title{On a Tip of Your Search: Evaluating Effect of Strategic Search Tips on User Success in Complex Informational Search Tasks}

%
% You need the command \numberofauthors to handle the 'placement
% and alignment' of the authors beneath the title.
%
% For aesthetic reasons, we recommend 'three authors at a time'
% i.e. three 'name/affiliation blocks' be placed beneath the title.
%
% NOTE: You are NOT restricted in how many 'rows' of
% "name/affiliations" may appear. We just ask that you restrict
% the number of 'columns' to three.
%
% Because of the available 'opening page real-estate'
% we ask you to refrain from putting more than six authors
% (two rows with three columns) beneath the article title.
% More than six makes the first-page appear very cluttered indeed.
%
% Use the \alignauthor commands to handle the names
% and affiliations for an 'aesthetic maximum' of six authors.
% Add names, affiliations, addresses for
% the seventh etc. author(s) as the argument for the
% \additionalauthors command.
% These 'additional authors' will be output/set for you
% without further effort on your part as the last section in
% the body of your article BEFORE References or any Appendices.

\numberofauthors{2} 

\author{
% 1st. author
\alignauthor
Denis Savenkov\\
       \affaddr{Emory University}\\
       \email{dsavenk@emory.edu}
% 2nd. author
\alignauthor
Eugene Agichtein\\
       \affaddr{Emory University}\\
       \email{eugene@mathcs.emory.edu}
}
\date{17 February 2014}
% Just remember to make sure that the TOTAL number of authors
% is the number that will appear on the first page PLUS the
% number that will appear in the \additionalauthors section.

\maketitle
\begin{abstract}
Research has found that the majority of cases when search engine users require assistance comes from the query formulation and refinement stages.
Providing users with tactical search feature tips was previously shown to increase search success rate and to have some educational effect.
In this paper we extend the study of the effect of search tips and focus on strategic tips, suggesting users a divide-and-conquer algorithm for solving difficult informational search tasks.
We prepared two sets of tips: task-specific, tailored to a particular search question and generic, describing a search strategy in general. 
The results of the conducted user study demonstrate the effectiveness of good search tips on search success rate.
However, tips that are too general and hard to follow can also be detrimental.
\end{abstract}

% A category with the (minimum) three required fields
\category{H.3.3}{Information storage and retrieval}{Information Search and Retrieval}[query formulation, search process]
%A category including the fourth, optional field follows...

\terms{Measurement, Design, Experimentation, Human Factors}

\keywords{User studies, search interface, experimental design, query reformulation, tactics, tips, suggestions, assistance, efficiency.}

\section{Introduction}
Search engines are ubiquitous and millions of people of varying experience use them on a daily basis.
But not all searches are successful.
Bilal and Kirby \cite{Bilal:2002:DSI:637512.637516} reported that about half of the participants of their users study were frustrated during their searches.
And \cite{xie2009understanding} explored different situations when users seek for help when performing a search task.
The study demonstrated that most of the time users have problems with formulating and refining their queries.
Which suggests that tuning search engine algorithms cannot solve all the problems users have.
Besides good retrieval performance a successful search requires users to possess certain skills.
Search skills can be trained. For example, Google offers a course\footnote{http://www.powersearchingwithgoogle.com} on improving search efficiency.
Although useful, such courses have very limited coverage as not all users are willing to spend their time watching videos on how to use web search more efficiently. 
Displaying search tips could be another technique that has some learning effect and offers immediate assistance to the user in solving her current search task.
The user study from \cite{Moraveji:2011:MIU:2009916.2009966} demonstrated that tactical search feature tips, suggesting to use a certain functionality of a search engine, help people find answers more quickly and the effect is retained after a week with tips removed.

Besides the awareness about search tools available, adopting general search strategies is extremely important when dealing with a difficult search task.
In this paper we focus on strategic search tips, that are designed to guide a user in solving her search problem.
To evaluate the effect of search tips on users behavior and success we conducted a user study in a form of a web search game.
Two sets of strategic tips were manually designed: one set featured task-specific tips describing a strategy to solve this particular search question, the other set described divide-and-conquer search strategy in general.
The results of the user study, described in this paper, demonstrate that well-designed task-specific strategic tips can improve search success rate.
However, generic tip, which was too general and harder to follow, had negative effect on user performance and satisfaction.

\section{Related Work}

There has been considerable amount of work on search assistance and improving user experience with feedback, suggestions and hints.
Interactive and human-computer information retrieval \cite{marchionini2006toward} focuses on interactions between users and search systems.
Graphical techniques can be used to visually represent large-scale collections of information and help searchers in their tasks \cite{card1999readings}.

Results of the study in \cite{xie2009understanding}, which focused on identification of different categories of help-seeking situations, demonstrates that in 41.5\% of the cases users were seeking for help to refine their searches followed by inability to construct search statements in 18\% of the cases, which confirms findings of \cite{Holscher2000337}.
Individual term (\cite{ruthven2003survey}) and query suggestion (\cite{Jones:2006:GQS:1135777.1135835},\cite{Bhatia:2011:QSA:2009916.2010023},\cite{Cao:2008:CQS:1401890.1401995}) are among the most popular techniques for helping users to augment their queries.
The study from \cite{Kelly:2009:CQT:1571941.1572006} demonstrated that users prefer query suggestions over term relevance feedback and that good manually designed suggestions improve retrieval performance.
Query suggestion methods usually use search logs to suggest a queries that are similar to the query of interest and work better for popular information needs \cite{Bhatia:2011:QSA:2009916.2010023}.

When query or term suggestions are not efficient, it is still possible to help users by providing potentially useful search tips.
An adaptive tool providing tactical search suggestions was presented in \cite{Kriewel2007} and users reported overall satisfaction with its automatic non-intrusive advices.
Modern search engines have many features that are not typically used by a average user, but can be very useful in particular situations as shown in \cite{Moraveji:2011:MIU:2009916.2009966}. The study demonstrated the potential effectiveness of tactical search feature tips and their teaching effect.
The major difference of this work from \cite{Moraveji:2011:MIU:2009916.2009966} is the type of search tips used.
Rather than suggesting users the available search functionality, this work focuses on strategic search tips, designed to solve difficult informational questions.
Many informational questions cannot be answered by a single web query and require splitting the task into pieces and combining partial answers into new searches. From our studies we noticed that users do not actively use this tactic and usually keep trying to reformulate their queries expecting to find the one that will give them the correct result.
% However, in these tasks it is hard to come up with a good hint which will help all the users.
% In this study we are evaluating the effect that task-specific manually designed strategic tips as well as generic task-independent tips might have on user experience and success rate.

% Not relevant
% The effect of different factors on users' perception of search task difficulty was studied in \cite{liu2011understanding}. The results demonstrated that users' prior expectations of task difficulty is likely to be inaccurate and perceived difficulty change before and after users worked on the task is affected by task types.

\section{User Study Description}

To estimate the effect of strategic search tips on users' behavior we conducted a study in a form of a web search game similar to a Google a Day\footnote{http://www.agoogleaday.com/} and uFindIt \cite{Ageev:2011:FYG:2009916.2009965}. Participants were hired using Amazon Machanical Turk\footnote{http://www.mturk.com/}. 

\subsection{Web Search Game}

\begin{figure}
\centering
\includegraphics[scale=0.29]{img/ufindit}
\caption{The interface of the search game used in the study}
\label{figure:ufindit}
\end{figure}

The goal of a web search game used in the user study is to find answers to several questions using web search.
The interface for the web search is provided on the same page.
Figure \ref{figure:ufindit} shows the interface of the game.

% Rules
At the beginning of a game users are instructed about the rules of the game.
The rules asks players to use the provided search interface only.
Answers to game questions are supposed to be found on a web page/pages, that was actually visited during search session.
In a rare occurrence that a user might know the answer to a question she is instructed to ignore the prior knowledge and use the search anyway.
Since tasks might be too difficult a chance to skip a question was provided, although users were instructed that effort put into solving a question is evaluated.
When the answer is found, a player provides it and a link to the answer web page.
The answer is automatically verified and a popup box notifies a player if the answer is incorrect (since the answer can be relatively long, presence of a keyword was checked).
A player can then continue searching or skip the question when she gives up.
Players are motivated to find the right answers with a bonus payment for answering all questions.

% Technology
We used API of one of the major search engines as a back-end of the game search interface.
All search results were cached so that users asking the same query get the same results.
Moreover, all links to web pages were rewritten to use our caching HTTP proxy.

% Finally
At the end of the game a questionnaire was presented asking for feedback on user satisfaction with the game, prior experience and other comments.

\subsection{Search Tasks Description}

The tasks for the study were borrowed from the ``a Google a Day'' questions archive.
Unfortunately, a lot of web pages discussing solutions to these questions exist.
So we had to filter search results and exclude all pages that mention a major part of the search question or ``a google a day'' phrase.
Since the questions are rather difficult, to keep users focused throughout the whole game we decided to limit the number of questions to 4.
The tasks are described in Table \ref{table:tasks}.
The order of the tasks was fixed to account for potential tiredness.

\begin{table*}
\centering
\caption{Search tasks used for the study}
\label{table:tasks}
\begin{tabular}{|p{1.55cm}|p{4.5cm}|p{4.5cm}|p{5cm}|} \hline
Task ID & Task Text & Answer & Specific tips \\ \hline
\parbox[t]{1cm}{Task 1\\(``hydra'')} & I can grow my body back in about two days if cut in half. Many scientists believe I do not undergo senescence. What am I? & Senescence means ``biological aging''. Hydra is considered biologically immortal and regenerates fast. & \parbox[t]{5cm}{
1. Find what is senescence\\
2. Find who do not undergo senescence\\
3. Find animals who can regenerate body and choose the one that satisfy both conditions} \\ \hline
\parbox[t]{1cm}{Task 2\\(``quirinus'')} & Of the Romans "group of three" gods in the Archaic Triad, which one did not have a Greek counterpart? & Archaic Triad includes Jupiter, Mars and Quirinus. Among those Quirinus didn't have a Greek counterpart. &
\parbox[t]{5cm}{
1. Find the names of the gods from the Archaic triad\\
2. For each of the gods find a Greek counterpart
}\\ \hline
\parbox[t]{1cm}{Task 3\\(``dinosaur'')} & As George surveyed the ``waterless place'', he unearthed some very important eggs of what animal? & Waterless place is the translation of the Mongolian word "Gobi" or ``Gobi Desert''. George Olsen found the first whole dinosaur eggs in 1923. & \parbox[t]{5cm}{
1. Find what is the ``waterless place'' mentioned in the question?\\
2. Search for important eggs discovery in this ``waterless place''}\\ \hline
\parbox[t]{1cm}{Task 4\\(``cherokee'')} & If you were in the basin of the Somme River at summers end in 1918, what language would you have had to speak to understand coded British communications? & Cherokee served as code talkers in the Second Battle of the Somme. & \parbox[t]{5cm}{
1. Find the name of the battle mentioned in the questions\\
2. Search for which coded communications language was used in this battle\\
} \\ \hline
\end{tabular}
\end{table*}

\subsection{Search Tips}
The questions used for the game are examples of difficult informational search tasks, which are hard to answer with the single search.
The questions have multiple parts and to solve them it is helpful to search for answers to parts of the questions and then combine them.
In one of the previous studies we noticed, that most of the users didn't adopt the divide-and-conquer strategy and kept reformulating trying the find the ``right'' query.
We decided to estimate the effect of strategic search tips, suggesting users to adopt the new strategy.

We built 2 sets of strategic tips: task specific and generic.
Task specific tips were constructed from one of the possible solutions to the questions and described one way to search and find the answer.
Specific tips for all questions are provided in Table \ref{table:tasks}.
Second set contained a single tip, which just described the strategy. The actual text displayed to the users is:
\vspace{-1mm}
\begin{enumerate} \itemsep0pt \parskip0pt \parsep0pt
\item Split question into 2 or more logical parts
\item Find answers to the parts of the question
\item Use answers to the parts of the question to find answer to the full question
\end{enumerate}
\vspace{-2mm}
For example, question: ``The second wife of King Henry VIII is said to haunt the grounds where she was executed. What does she supposedly have tucked under her arm?''
\vspace{-2mm}
\begin{enumerate} \itemsep0pt \parskip0pt \parsep0pt
\item Search [second wife King Henry VIII] to find Anne Boleyn.
\item Search [Anne Boleyn under arm] to find that her ghost is in the London Tower where she is said to carry her head tucked underneath her arm.
\end{enumerate}
\vspace{-2mm}
To control for the learning effect demonstrated in \cite{Moraveji:2011:MIU:2009916.2009966} each user was assigned to one of the three groups: a group who didn't get any tips, those who got task-specific tips and those who got the generic tip.
During the game the tips were displayed for each question all the time in the right panel of the search interface as demonstrated on Figure \ref{figure:ufindit}.

\section{Results}
From 199 participants, who clicked on the HIT on Amazon Mechanical Turk, only 169 accepted the rules of the game and got to the first question.
Furthermore, a fraction of users decided to give up.
Thus only 90 players finished the game, from those there were 9 submissions which we filtered out from the future analysis, because
of the lack of effort (e.g. some players skipped several tasks after only a single query) or usage of external resources (e.g. external search engine).
From 81 submissions 10 players indicated in the survey that they didn't see the tips which were shown to them, so we further filtered those submissions and finally we had 71 completed games (29 for no tips, 20 for task-specific tips and 22 for generic tip groups).


\subsection{Analysis}

The main characteristic of interest was the search success rate measured by the number of questions answered correctly by users in different groups\footnote{Since users were allowed to skip the question we are counting the number of questions that were eventually solved correctly even if a player made some incorrect attempts}.
Figure \ref{figure:task_success} plots the fraction of correct answers for each group.
As you can see, success rate is higher for users who saw task-specific tips compared to users who didn't get such assistance.
Surprisingly, having the generic tip decreased the success rate, although users could easily ignore the tip they didn't like.
A possible explanation is: generic tip was harder to follow and users who tried and failed became frustrated and didn't restart their searches.

\begin{figure}[ht]
\centering
\includegraphics[scale=0.4]{img/success_per_task}
\caption{Success rate per task for each group of participants}
\label{figure:task_success}
\end{figure}

Similar to \cite{Moraveji:2011:MIU:2009916.2009966} we looked at the average time to answer a question (for this analysis we removed games where a user didn't find the answer and skipped the task).
The plot on Figure \ref{figure:task_time} doesn't show an improvement for the task-specific tips group, except for the question 1.
The task-specific tips we gave represent a possible way to solve the problem and there is no guarantee, that it is the optimal one.
It is worth noting, that users from the generic search tip group had slightly higher variance in success time, which can probably be explained by the fact that some users were successful in finding the right way to follow the tip and some other users struggled with it much longer.

\begin{figure}[ht]
\centering
\includegraphics[scale=0.4]{img/time_per_task}
\caption{Task completion time for each group of players}
\label{figure:task_time}
\end{figure}

Another interesting insight comes from the number of incorrect attempts users made.
Figure \ref{figure:incorrect} demonstrates the average number of incorrect submissions for all groups of users.
Although the variance is high, but there is a tendency, that users who saw task-specific tips made less submission attempts and users who saw generic tip were incorrect slightly more often than users who didn't get any tips during the game. 
This is not in direct correspondence with time spent on the game.
Thus users who saw a clear strategy to solve the question were less likely to notice plausible, but incorrect solution.
Moreover, we analyzed texts of incorrect answers and can conclude that a big part of incorrect submission are due to users trying all possible options they found on the way even if these options are clearly wrong.
We should note, that unfortunately we didn't limit the number of attempts per problem, thus strategy to verify an answer by submitting it made sense.

\begin{figure}[ht]
\centering
\includegraphics[scale=0.4]{img/incorrect}
\caption{The number of incorrect submission attempts per question for all groups of users}
\label{figure:incorrect}
\end{figure}

We looked at some other search behavior characteristics: number of queries submitted, number of clicks made, average length of the queries. The variance in these characteristics was too high to make any speculations regarding their meaning.

Finally, we looked at the surveys filled by each group of users.
Figure \ref{figure:survey} presents proportions of different answers to three of the questions: ``How did you like the game?'', ``How difficult was the game?'' and ``Were search tips useful to you?''.
Surprisingly, results for the first question were lower for users who saw tips during the game and users who didn't saw tips liked the game more.
It seems that solving questions with the tips was less enjoyable than without assistance.
The answers to the question about game difficulty are in agreement with the success rate: users who saw task-specific tips rate game easier than participants who struggled to find the correct answers.
The game was rather very difficult on average, however, some participants from the group who received task-specific tips surprisingly rated it as very easy, which suggests that our tips do help users.
This is supported by the answers to the last question on whether tips were helpful (Figure \ref{figure:survey:useful}).

\begin{figure*}[ht]
\centering
\begin{subfigure}[t]{0.3\textwidth}
	\includegraphics[scale=0.26]{img/liked}
	\caption{How did you like the game?}
    \label{figure:survey:liked}
\end{subfigure}
\begin{subfigure}[t]{0.3\textwidth}
	\includegraphics[scale=0.26]{img/difficult}
	\caption{How difficult was the game?}
    \label{figure:survey:difficult}
\end{subfigure}
\begin{subfigure}[t]{0.3\textwidth}
	\includegraphics[scale=0.26]{img/useful}
	\caption{Were search tips useful to you?}
    \label{figure:survey:useful}
\end{subfigure}
\caption{Proportions of replies to some of the survey question for each group of users}
\label{figure:survey}
\end{figure*}

\section{Conclusion}
In this paper we studied the effect of strategic search tips on user behavior.
The conducted user study in a form of a web search game demonstrated the potential of good tips in improving search success rate.
However, to be useful, they should be designed carefully.
Search tips that are difficult to follow or have variety of options can be detrimental to search success.

As a future work it would be interesting to verify the learning effect of strategic tips similar to what \cite{Moraveji:2011:MIU:2009916.2009966} proved for tactical search feature tips.
But the main question of future work is how to generate useful search tips automatically.
It should be possible to learn strategies applied by the experienced search users and suggest them to the rest.

%ACKNOWLEDGMENTS are optional
\section{Acknowledgments}
The authors would like to thank Daniel Russel for providing an archive of questions from ``a Google a Day'' search game.

%
% The following two commands are all you need in the
% initial runs of your .tex file to
% produce the bibliography for the citations in your paper.
\bibliographystyle{abbrv}
\bibliography{sigproc}  % sigproc.bib is the name of the Bibliography in this case
% You must have a proper ".bib" file
%  and remember to run:
% latex bibtex latex latex
% to resolve all references
%
% ACM needs 'a single self-contained file'!
%
%APPENDICES are optional
%\balancecolumns

\end{document}
